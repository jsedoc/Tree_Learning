# (1/24)
Deep learning book 10 -10.2.1 , 10.2.3 - 10.6, 10.10

Sequence-to-sequence paper https://arxiv.org/abs/1409.3215

pytorch implementation of http://gluon.mxnet.io/chapter05_recurrent-neural-networks/simple-rnn.html#Recurrent-neural-networks

hyperparameter searching -  

Google Vizier
paper on Vizier https://research.google.com/pubs/pub46180.html

blog on Vizier https://cloud.google.com/blog/big-data/2017/08/hyperparameter-tuning-in-cloud-machine-learning-engine-using-bayesian-optimization

application to NLP https://openreview.net/pdf?id=ByJHuTgA-

Hyperopt 
http://jaberg.github.com/hyperopt


---
# (1/30)

Cho attention https://arxiv.org/abs/1409.0473

Attention paper  - https://arxiv.org/pdf/1508.04025.pdf

Parameter initialization - Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of AISTATS

useful blog http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
