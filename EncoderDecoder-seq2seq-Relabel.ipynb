{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from queue import *\n",
    "\n",
    "from seq2seqLoader import *\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "[111, 109, 109, 109, 109, 101, 60, 8, 110, 95, 109, 89, 62, 100, 110, 110, 92, 109, 109, 109, 99, 83, 10, 110, 58, 98, 110, 63, 87, 110, 110, 16, 109, 109, 109, 97, 50, 64, 110, 46, 52, 110, 43, 96, 110, 110, 112]\n",
      "[115, 113, 113, 113, 113, 101, 112, 8, 114, 111, 113, 89, 112, 100, 114, 114, 111, 113, 113, 113, 99, 111, 10, 114, 112, 98, 114, 112, 87, 114, 114, 109, 113, 113, 113, 97, 112, 64, 114, 110, 52, 114, 110, 96, 114, 114, 116]\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"/data2/t2t/synth_data/relabel/train.orig\"\n",
    "target_filename = \"/data2/t2t/synth_data/relabel/train.interior_relabel\"\n",
    "data_loader = T2TDataLoader(train_filename,target_filename)\n",
    "training_data = data_loader.get_data()\n",
    "print(len(training_data))\n",
    "training_pairs = [(random.choice(training_data))\n",
    "                  for i in range(len(training_data))]\n",
    "print(training_pairs[0][0])\n",
    "print(training_pairs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(input_size, embedding_size)    \n",
    "        self.lstm = nn.LSTM(input_size = embedding_size, hidden_size = hidden_size,num_layers = 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, c):\n",
    "        embedded = self.word_embeddings(input).view(1, 1, -1)\n",
    "\n",
    "        output = embedded\n",
    "        output,(hidden,c) = self.lstm(output, (hidden, c))\n",
    "        return output,hidden,c\n",
    "\n",
    "    def initCells(self):\n",
    "        \n",
    "        if use_cuda:\n",
    "            return Variable(torch.zeros(1, 1, hidden_size).cuda(gpu_no))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, 1, hidden_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda(gpu_no)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initCells()\n",
    "    encoder_c = encoder.initCells()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = len(input_variable)\n",
    "    target_length = len(target_variable)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    if use_cuda:\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden,encoder_c = encoder(input_variable[ei], encoder_hidden,encoder_c)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            target_variable[di], decoder_hidden)\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = Variable(torch.LongTensor(training_pair[0]).view(-1, 1))\n",
    "        target_variable = Variable(torch.LongTensor(training_pair[1]).view(-1, 1))\n",
    "        \n",
    "        if use_cuda:\n",
    "            input_variable = input_variable.cuda(gpu_no)\n",
    "            target_variable = target_variable.cuda(gpu_no)\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "            \n",
    "            torch.save(encoder.state_dict(), './pickles/encoder_seq2seq.pth')\n",
    "            torch.save(decoder.state_dict(), './pickles/decoder_seq2seq.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 14s (- 24m 26s) (100 1%) 1.2606\n",
      "0m 28s (- 22m 55s) (200 2%) 0.5848\n",
      "0m 43s (- 23m 30s) (300 3%) 0.2365\n",
      "0m 56s (- 22m 31s) (400 4%) 0.1183\n",
      "1m 12s (- 22m 56s) (500 5%) 0.0502\n",
      "1m 25s (- 22m 23s) (600 6%) 0.0256\n",
      "1m 40s (- 22m 14s) (700 7%) 0.0181\n",
      "1m 53s (- 21m 43s) (800 8%) 0.0125\n",
      "2m 7s (- 21m 29s) (900 9%) 0.0123\n",
      "2m 20s (- 21m 0s) (1000 10%) 0.0091\n",
      "2m 33s (- 20m 43s) (1100 11%) 0.0075\n",
      "2m 48s (- 20m 33s) (1200 12%) 0.0068\n",
      "3m 3s (- 20m 28s) (1300 13%) 0.0064\n",
      "3m 18s (- 20m 17s) (1400 14%) 0.0055\n",
      "3m 32s (- 20m 2s) (1500 15%) 0.0046\n",
      "3m 45s (- 19m 46s) (1600 16%) 0.0043\n",
      "3m 59s (- 19m 27s) (1700 17%) 0.0041\n",
      "4m 15s (- 19m 21s) (1800 18%) 0.0035\n",
      "4m 28s (- 19m 6s) (1900 19%) 0.0033\n",
      "4m 40s (- 18m 43s) (2000 20%) 0.0034\n",
      "4m 55s (- 18m 30s) (2100 21%) 0.0031\n",
      "5m 7s (- 18m 10s) (2200 22%) 0.0028\n",
      "5m 23s (- 18m 2s) (2300 23%) 0.0025\n",
      "5m 36s (- 17m 46s) (2400 24%) 0.0026\n",
      "5m 50s (- 17m 31s) (2500 25%) 0.0027\n",
      "6m 6s (- 17m 22s) (2600 26%) 0.0021\n",
      "6m 20s (- 17m 8s) (2700 27%) 0.0024\n",
      "6m 35s (- 16m 57s) (2800 28%) 0.0020\n",
      "6m 49s (- 16m 42s) (2900 28%) 0.0022\n",
      "7m 4s (- 16m 30s) (3000 30%) 0.0020\n",
      "7m 18s (- 16m 16s) (3100 31%) 0.0020\n",
      "7m 33s (- 16m 3s) (3200 32%) 0.0018\n",
      "7m 48s (- 15m 51s) (3300 33%) 0.0017\n",
      "8m 1s (- 15m 33s) (3400 34%) 0.0017\n",
      "8m 15s (- 15m 20s) (3500 35%) 0.0016\n",
      "8m 28s (- 15m 3s) (3600 36%) 0.0016\n",
      "8m 42s (- 14m 49s) (3700 37%) 0.0015\n",
      "8m 56s (- 14m 35s) (3800 38%) 0.0014\n",
      "9m 9s (- 14m 19s) (3900 39%) 0.0015\n",
      "9m 23s (- 14m 5s) (4000 40%) 0.0013\n",
      "9m 37s (- 13m 51s) (4100 41%) 0.0013\n",
      "9m 51s (- 13m 36s) (4200 42%) 0.0013\n",
      "10m 6s (- 13m 23s) (4300 43%) 0.0014\n",
      "10m 20s (- 13m 9s) (4400 44%) 0.0022\n",
      "10m 34s (- 12m 55s) (4500 45%) 0.0011\n",
      "10m 48s (- 12m 40s) (4600 46%) 0.0012\n",
      "11m 3s (- 12m 27s) (4700 47%) 0.0011\n",
      "11m 18s (- 12m 14s) (4800 48%) 0.0011\n",
      "11m 33s (- 12m 1s) (4900 49%) 0.0010\n",
      "11m 49s (- 11m 49s) (5000 50%) 0.0010\n",
      "12m 2s (- 11m 34s) (5100 51%) 0.0010\n",
      "12m 18s (- 11m 21s) (5200 52%) 0.0010\n",
      "12m 30s (- 11m 5s) (5300 53%) 0.0010\n",
      "12m 44s (- 10m 50s) (5400 54%) 0.0010\n",
      "12m 55s (- 10m 34s) (5500 55%) 0.0009\n",
      "13m 7s (- 10m 19s) (5600 56%) 0.0009\n",
      "13m 22s (- 10m 5s) (5700 56%) 0.0009\n",
      "13m 34s (- 9m 50s) (5800 57%) 0.0008\n",
      "13m 50s (- 9m 36s) (5900 59%) 0.0008\n",
      "14m 2s (- 9m 21s) (6000 60%) 0.0009\n",
      "14m 15s (- 9m 6s) (6100 61%) 0.0008\n",
      "14m 29s (- 8m 53s) (6200 62%) 0.0008\n",
      "14m 42s (- 8m 38s) (6300 63%) 0.0009\n",
      "14m 56s (- 8m 24s) (6400 64%) 0.0008\n",
      "15m 10s (- 8m 10s) (6500 65%) 0.0008\n",
      "15m 24s (- 7m 56s) (6600 66%) 0.0008\n",
      "15m 38s (- 7m 42s) (6700 67%) 0.0007\n",
      "15m 53s (- 7m 28s) (6800 68%) 0.0007\n",
      "16m 6s (- 7m 14s) (6900 69%) 0.0007\n",
      "16m 21s (- 7m 0s) (7000 70%) 0.0007\n",
      "16m 35s (- 6m 46s) (7100 71%) 0.0007\n",
      "16m 50s (- 6m 33s) (7200 72%) 0.0007\n",
      "17m 4s (- 6m 18s) (7300 73%) 0.0007\n",
      "17m 18s (- 6m 4s) (7400 74%) 0.0007\n",
      "17m 33s (- 5m 51s) (7500 75%) 0.0015\n",
      "17m 45s (- 5m 36s) (7600 76%) 0.0007\n",
      "18m 1s (- 5m 22s) (7700 77%) 0.0006\n",
      "18m 13s (- 5m 8s) (7800 78%) 0.0006\n",
      "18m 28s (- 4m 54s) (7900 79%) 0.0006\n",
      "18m 41s (- 4m 40s) (8000 80%) 0.0006\n",
      "18m 54s (- 4m 26s) (8100 81%) 0.0006\n",
      "19m 9s (- 4m 12s) (8200 82%) 0.0006\n",
      "19m 21s (- 3m 57s) (8300 83%) 0.0006\n",
      "19m 36s (- 3m 44s) (8400 84%) 0.0006\n",
      "19m 49s (- 3m 29s) (8500 85%) 0.0006\n",
      "20m 4s (- 3m 16s) (8600 86%) 0.0005\n",
      "20m 19s (- 3m 2s) (8700 87%) 0.0005\n",
      "20m 33s (- 2m 48s) (8800 88%) 0.0005\n",
      "20m 46s (- 2m 34s) (8900 89%) 0.0006\n",
      "21m 0s (- 2m 20s) (9000 90%) 0.0005\n",
      "21m 15s (- 2m 6s) (9100 91%) 0.0005\n",
      "21m 28s (- 1m 52s) (9200 92%) 0.0005\n",
      "21m 44s (- 1m 38s) (9300 93%) 0.0005\n",
      "21m 58s (- 1m 24s) (9400 94%) 0.0005\n",
      "22m 13s (- 1m 10s) (9500 95%) 0.0005\n",
      "22m 26s (- 0m 56s) (9600 96%) 0.0005\n",
      "22m 40s (- 0m 42s) (9700 97%) 0.0005\n",
      "22m 53s (- 0m 28s) (9800 98%) 0.0005\n",
      "23m 9s (- 0m 14s) (9900 99%) 0.0005\n",
      "23m 23s (- 0m 0s) (10000 100%) 0.0012\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "embedding_size = 256\n",
    "gpu_no = 1\n",
    "# 0-99,+,-,*,/,(,)\n",
    "encoder_vocab_size = 113\n",
    "decoder_vocab_size = 117\n",
    "use_cuda = False\n",
    "encoder1 = EncoderLSTM(encoder_vocab_size, hidden_size,embedding_size)\n",
    "decoder1 = DecoderRNN(hidden_size, decoder_vocab_size)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda(gpu_no)\n",
    "    decoder1 = decoder1.cuda(gpu_no)\n",
    "\n",
    "trainIters(encoder1,decoder1, 10000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
