{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from hyperopt import pyll, hp, fmin, tpe\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4489\n",
      "eng 2925\n",
      "[u'je suis satisfait de sa performance .', u'i m pleased with his performance .']\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        \n",
    "        #self.Wxh = nn.Parameter(torch.zeros(1,1,256)).cuda()\n",
    "        \n",
    "        #self.Wxh = nn.Linear(256,256)\n",
    "        #self.Whh = nn.Linear(256,256)\n",
    "        \n",
    "        #self.Why = nn.Linear(256,256)\n",
    "        \n",
    "        self.rnn = nn.RNN(hidden_size,hidden_size,1)\n",
    "        \n",
    "#         self.Whh =nn.Parameter(torch.zeros(1,1,256)).cuda()\n",
    "#         self.bh = nn.Parameter(torch.zeros(1,1,256)).cuda()\n",
    "#         self.Why = nn.Parameter(torch.zeros(1,1,256)).cuda()\n",
    "#         self.by = nn.Parameter(torch.zeros(1,1,256)).cuda()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "\n",
    "       \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        output = embedded\n",
    "        \n",
    "        #output = output[0][0].view(1,256)\n",
    "        hidden = hidden.view(1,1,-1)\n",
    "        \n",
    "#         balh = self.Wxh(output[0][0].view(1,256))\n",
    "        \n",
    "#         hidden = self.tanh(self.Wxh(output[0][0].view(1,256))+self.Whh(hidden)).view(1,256)\n",
    "        \n",
    "#         output = self.softmax((self.Why(hidden))).view(1,1,256)\n",
    "\n",
    "        #print(\"Embedding size: \",output.size())\n",
    "        #print(\"Hidden size: \",hidden.size())\n",
    "        \n",
    "        output,hidden = self.rnn(output,hidden)\n",
    "        \n",
    "        #output = output.view(-1,256)\n",
    "        \n",
    "        \n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda(1)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.rnn = nn.RNN(hidden_size,hidden_size,1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda(1)\n",
    "        else:\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda(1)\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda(1) if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda(1) if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden.view(1,1,-1)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda(1) if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=1000, learning_rate=0.01,optimizer='SGD'):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    loss_initial = 0\n",
    "    for iter in range(1, n_iters + 1):\n",
    "       \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "        \n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        \n",
    "            \n",
    "        print_loss_total += loss\n",
    "        \n",
    "\n",
    "        \n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            \n",
    "            if iter == 1000:\n",
    "                loss_initial = print_loss_total\n",
    "            \n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "                                         \n",
    "    return print_loss_avg/loss_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 18s (- 0m 18s) (1000 50%) 3.4969\n",
      "0m 37s (- 0m 0s) (2000 100%) 3.1317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1116.6158522903122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "hidden_size = 300\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda(1)\n",
    "    decoder1 = decoder1.cuda(1)\n",
    "\n",
    "for name, p in encoder1.named_parameters():\n",
    "    if len(p.size()) >= 2:\n",
    "        init.uniform(p,-0.08,0.08)\n",
    "\n",
    "trainIters(encoder1,decoder1, 2000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(params):\n",
    "\n",
    "    hidden_size = params['hidden_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    weights_init = params['weights_init']\n",
    "    optimizer = params['optimizer']\n",
    "    \n",
    "    encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "    decoder1 = DecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "    if use_cuda:\n",
    "        encoder1 = encoder1.cuda(1)\n",
    "        decoder1 = decoder1.cuda(1)\n",
    "\n",
    "    for name, p in encoder1.named_parameters():\n",
    "        if len(p.size()) >= 2:\n",
    "            init.uniform(p,-weights_init,weights_init)\n",
    "\n",
    "    return trainIters(encoder1,decoder1, 75000, print_every=1000,learning_rate=learning_rate,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "space = {'weights_init': hp.uniform('weights_init', -0.1, 0.1) ,\n",
    "'hidden_size':scope.int(hp.quniform('hidden_size',500,4000,50)),\n",
    "'learning_rate':hp.uniform('learning_rate',0.005, 0.05),\n",
    "'optimizer':hp.choice('optimizer_name',('Adam', 'SGD'))\n",
    "}\n",
    "\n",
    "best = fmin(main, space, algo=tpe.suggest, max_evals=5)\n",
    "print(\"best parameters\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda(1) if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda(1) if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden.view(1,1,-1)\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda(1) if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis une bonne personne .\n",
      "= i m a good person .\n",
      "< i m not of . . <EOS>\n",
      "\n",
      "> il lit .\n",
      "= he is reading .\n",
      "< he s a . <EOS>\n",
      "\n",
      "> je suis fatigue de danser .\n",
      "= i m tired of dancing .\n",
      "< i m not of . . <EOS>\n",
      "\n",
      "> nous sommes des nouveaux venus .\n",
      "= we re newcomers .\n",
      "< we re all here . <EOS>\n",
      "\n",
      "> vous etes tres raffinee .\n",
      "= you re very sophisticated .\n",
      "< you are very understanding . <EOS>\n",
      "\n",
      "> je suis desolee mon pere est dehors .\n",
      "= i m sorry my father is out .\n",
      "< i m very proud of our . <EOS>\n",
      "\n",
      "> tu es timide .\n",
      "= you re shy .\n",
      "< you re sick . <EOS>\n",
      "\n",
      "> elle est une championne de football .\n",
      "= she s a soccer champion .\n",
      "< she s a at of . <EOS>\n",
      "\n",
      "> je creve d envie de la revoir .\n",
      "= i am dying to see her again .\n",
      "< i m very proud of our . <EOS>\n",
      "\n",
      "> je me sens assez confiant .\n",
      "= i m feeling pretty confident .\n",
      "< i m not of . . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('embedding.weight', Parameter containing:\n",
       "  -1.4506e+00 -1.1463e+00 -1.1788e-01  ...  -1.4200e+00 -7.5380e-01 -8.4167e-01\n",
       "   9.7898e-01  6.0189e-01  1.0324e+00  ...   6.2386e-01  1.1707e+00  6.2751e-01\n",
       "   3.3029e-02 -2.4832e-01  1.0357e-01  ...  -1.0030e+00 -1.4789e+00 -4.5356e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -7.3199e-01 -1.4512e+00  2.0953e-01  ...   3.9212e-01  6.5632e-01  6.7086e-01\n",
       "  -7.1268e-01  1.0285e+00  1.2198e+00  ...   9.6443e-02  4.6183e-01 -9.8866e-01\n",
       "  -1.5849e-01  9.2358e-01 -2.7277e+00  ...   1.3562e+00 -1.6801e+00  9.9849e-01\n",
       "  [torch.cuda.FloatTensor of size 4489x256 (GPU 0)]),\n",
       " ('Wxh.weight', Parameter containing:\n",
       "   3.9429e-02 -5.3712e-02 -4.0545e-02  ...  -2.1362e-02 -3.2580e-02  1.9622e-04\n",
       "   5.2920e-02  2.2854e-02  4.1614e-02  ...   5.9415e-02 -5.4396e-02 -5.1029e-02\n",
       "   5.8379e-02 -2.6421e-02 -4.6973e-02  ...   4.5297e-02  4.7184e-02 -4.4916e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "   4.4201e-03  4.9569e-02 -1.0789e-03  ...   2.3370e-02  2.2681e-02  3.3191e-02\n",
       "   4.1736e-02 -4.9328e-02 -8.4260e-03  ...  -9.8384e-03  2.2815e-02  4.3324e-02\n",
       "  -2.1005e-02  3.5103e-02 -2.6400e-02  ...   4.6589e-02  2.1288e-02 -2.5510e-02\n",
       "  [torch.cuda.FloatTensor of size 256x256 (GPU 0)]),\n",
       " ('Wxh.bias', Parameter containing:\n",
       "  1.00000e-02 *\n",
       "   -2.0600\n",
       "    0.4427\n",
       "    0.7227\n",
       "    1.5613\n",
       "   -0.6471\n",
       "   -5.2086\n",
       "   -3.9652\n",
       "    5.6686\n",
       "    6.1804\n",
       "    0.0008\n",
       "    4.3803\n",
       "    2.6316\n",
       "   -1.8853\n",
       "   -5.2620\n",
       "    0.5291\n",
       "    6.1064\n",
       "    0.0888\n",
       "    3.3175\n",
       "    4.3090\n",
       "    4.2693\n",
       "    2.5515\n",
       "   -1.1997\n",
       "    0.5841\n",
       "    2.3289\n",
       "   -2.3683\n",
       "   -2.0944\n",
       "   -1.6548\n",
       "    4.2373\n",
       "   -1.8404\n",
       "    3.7433\n",
       "   -5.9730\n",
       "   -4.5217\n",
       "    4.2161\n",
       "   -2.1907\n",
       "   -2.2787\n",
       "   -5.0798\n",
       "    4.1879\n",
       "   -5.3361\n",
       "   -2.4686\n",
       "   -0.4292\n",
       "   -4.7498\n",
       "    4.3943\n",
       "    4.0174\n",
       "    1.0097\n",
       "   -4.4675\n",
       "    0.8649\n",
       "   -1.2924\n",
       "    0.4885\n",
       "    3.3738\n",
       "    5.4476\n",
       "    1.6739\n",
       "   -0.7444\n",
       "    1.5236\n",
       "    0.4403\n",
       "   -2.4468\n",
       "   -0.5704\n",
       "    3.8988\n",
       "    1.5981\n",
       "   -5.7658\n",
       "    3.6093\n",
       "    3.9846\n",
       "    1.6390\n",
       "   -0.9580\n",
       "   -0.9879\n",
       "    5.4479\n",
       "   -2.7693\n",
       "    2.8315\n",
       "    0.4492\n",
       "   -0.4099\n",
       "   -3.6815\n",
       "    3.7194\n",
       "    1.0733\n",
       "   -0.9571\n",
       "    5.2292\n",
       "   -1.7227\n",
       "   -4.1483\n",
       "    2.6589\n",
       "    2.7598\n",
       "    0.1150\n",
       "   -4.6510\n",
       "    0.7826\n",
       "   -0.8262\n",
       "   -6.1506\n",
       "    5.1194\n",
       "   -1.2379\n",
       "    6.2873\n",
       "   -2.2020\n",
       "   -1.0695\n",
       "   -4.1328\n",
       "   -3.0675\n",
       "    4.8450\n",
       "    0.0687\n",
       "    5.9295\n",
       "   -5.4600\n",
       "    3.9423\n",
       "   -2.3117\n",
       "   -5.8796\n",
       "    5.4598\n",
       "    3.1622\n",
       "   -5.8818\n",
       "   -2.7585\n",
       "   -0.0157\n",
       "   -1.8431\n",
       "    3.4789\n",
       "   -5.5439\n",
       "    4.0716\n",
       "    5.7832\n",
       "   -2.7893\n",
       "    4.3226\n",
       "    0.0577\n",
       "    3.3772\n",
       "    1.4426\n",
       "    1.4916\n",
       "    1.7533\n",
       "    1.7473\n",
       "    6.0348\n",
       "   -6.1668\n",
       "   -4.9697\n",
       "    1.5036\n",
       "    5.3962\n",
       "    2.5130\n",
       "   -1.8969\n",
       "   -1.6586\n",
       "    0.6765\n",
       "    5.7964\n",
       "    1.7869\n",
       "    0.1568\n",
       "    5.8616\n",
       "   -4.8198\n",
       "   -2.9830\n",
       "   -1.9249\n",
       "    2.9449\n",
       "    3.6247\n",
       "    2.8613\n",
       "   -2.4722\n",
       "    0.2395\n",
       "    2.4566\n",
       "    3.7482\n",
       "    3.2369\n",
       "    0.1508\n",
       "   -5.3523\n",
       "   -1.6014\n",
       "    5.1075\n",
       "   -2.8328\n",
       "   -0.3101\n",
       "    3.8899\n",
       "   -5.8006\n",
       "   -0.3331\n",
       "   -3.4387\n",
       "   -3.9007\n",
       "    5.7366\n",
       "   -0.1351\n",
       "   -5.1330\n",
       "    1.4690\n",
       "   -2.5606\n",
       "   -4.5743\n",
       "    5.1292\n",
       "   -2.7578\n",
       "    4.4934\n",
       "   -2.6363\n",
       "    5.8598\n",
       "   -3.4323\n",
       "    0.3837\n",
       "    5.5196\n",
       "   -2.8238\n",
       "   -1.7825\n",
       "    2.7578\n",
       "    4.9507\n",
       "   -2.0954\n",
       "   -2.9652\n",
       "   -0.5724\n",
       "    1.5931\n",
       "   -2.8926\n",
       "   -5.6282\n",
       "   -5.7427\n",
       "    1.2522\n",
       "    1.1845\n",
       "   -5.7072\n",
       "    3.5809\n",
       "   -5.9898\n",
       "    5.1425\n",
       "   -1.4789\n",
       "    2.7774\n",
       "   -5.5648\n",
       "    0.4542\n",
       "    0.1561\n",
       "    5.9417\n",
       "   -2.0949\n",
       "   -5.1431\n",
       "   -5.4464\n",
       "    6.1660\n",
       "    5.4005\n",
       "    4.0621\n",
       "   -6.0548\n",
       "    1.5733\n",
       "   -1.9835\n",
       "   -5.1959\n",
       "    3.8175\n",
       "   -5.0122\n",
       "   -3.9209\n",
       "    3.4853\n",
       "   -5.4082\n",
       "   -2.0437\n",
       "   -5.2878\n",
       "   -3.4131\n",
       "    2.6459\n",
       "    1.5329\n",
       "   -0.7121\n",
       "    3.7797\n",
       "    2.2498\n",
       "    2.5478\n",
       "    0.7588\n",
       "    4.4313\n",
       "   -4.6506\n",
       "   -1.5593\n",
       "   -4.7711\n",
       "   -4.2890\n",
       "    3.7696\n",
       "    1.8990\n",
       "   -5.1049\n",
       "   -1.7670\n",
       "   -5.4199\n",
       "   -2.0713\n",
       "    5.0338\n",
       "    1.3983\n",
       "    0.2626\n",
       "    6.1038\n",
       "   -4.6778\n",
       "    5.4489\n",
       "    0.6744\n",
       "    3.3859\n",
       "    3.2442\n",
       "    3.4503\n",
       "   -5.7823\n",
       "    0.8963\n",
       "    1.2832\n",
       "   -0.1336\n",
       "   -6.1644\n",
       "    5.3747\n",
       "    2.2394\n",
       "    5.4560\n",
       "   -4.2692\n",
       "    1.7010\n",
       "   -5.9822\n",
       "    0.6793\n",
       "    4.8681\n",
       "   -6.1123\n",
       "   -2.4503\n",
       "    3.8601\n",
       "   -5.0806\n",
       "   -6.1634\n",
       "   -4.4983\n",
       "    1.9967\n",
       "    3.3255\n",
       "   -3.9172\n",
       "    5.1962\n",
       "  [torch.cuda.FloatTensor of size 256 (GPU 0)]),\n",
       " ('Whh.weight', Parameter containing:\n",
       "   1.4770e-03  5.1417e-02  5.4320e-02  ...  -1.7705e-03  3.8435e-03 -8.8912e-03\n",
       "   4.2382e-02  3.2708e-02 -4.4723e-02  ...   5.7712e-02 -4.7947e-02 -7.7471e-03\n",
       "   4.1406e-03 -1.6778e-02  1.2674e-02  ...   5.2585e-02 -2.7548e-02  1.0158e-03\n",
       "                  ...                   ⋱                   ...                \n",
       "  -1.6381e-02 -1.1697e-02 -3.0849e-02  ...   3.7323e-02 -6.0745e-02  1.6340e-02\n",
       "  -1.9851e-02 -5.1559e-02 -6.0067e-02  ...   2.5876e-02  3.4510e-02  4.0910e-02\n",
       "  -1.4826e-02  5.0192e-02  3.0052e-02  ...   3.6223e-02  2.8507e-02 -4.8300e-02\n",
       "  [torch.cuda.FloatTensor of size 256x256 (GPU 0)]),\n",
       " ('Whh.bias', Parameter containing:\n",
       "  1.00000e-02 *\n",
       "    5.3508\n",
       "    4.3523\n",
       "    5.7313\n",
       "   -4.1732\n",
       "   -4.0668\n",
       "    5.8684\n",
       "    1.9918\n",
       "   -1.6718\n",
       "    1.2913\n",
       "    1.3883\n",
       "    5.7418\n",
       "    5.0695\n",
       "    4.3859\n",
       "    4.0668\n",
       "    1.9108\n",
       "    6.1294\n",
       "   -4.9004\n",
       "   -5.8172\n",
       "   -1.7701\n",
       "   -2.3184\n",
       "    4.8782\n",
       "    6.0729\n",
       "    2.7040\n",
       "    0.4616\n",
       "   -1.3236\n",
       "   -0.3887\n",
       "   -5.1256\n",
       "    1.5269\n",
       "   -3.4270\n",
       "    5.7536\n",
       "    2.5898\n",
       "   -3.4210\n",
       "   -0.6607\n",
       "   -4.0840\n",
       "    3.5879\n",
       "    5.8982\n",
       "   -0.8824\n",
       "   -3.0931\n",
       "    5.6301\n",
       "   -4.0466\n",
       "   -0.3082\n",
       "    0.0954\n",
       "   -3.1031\n",
       "   -5.8215\n",
       "   -1.9986\n",
       "   -5.9485\n",
       "    4.2456\n",
       "   -1.8166\n",
       "   -4.6927\n",
       "    1.2499\n",
       "    0.8249\n",
       "   -3.0222\n",
       "    0.3786\n",
       "   -1.4450\n",
       "    5.5256\n",
       "    4.9532\n",
       "    4.0561\n",
       "    2.5678\n",
       "    0.1565\n",
       "   -0.9894\n",
       "    6.1206\n",
       "   -0.5086\n",
       "    4.3686\n",
       "    0.0160\n",
       "   -2.1737\n",
       "   -2.9760\n",
       "   -0.9316\n",
       "   -2.1841\n",
       "   -0.3994\n",
       "    1.0066\n",
       "    0.4837\n",
       "    0.4605\n",
       "   -5.9214\n",
       "   -1.3304\n",
       "   -1.7104\n",
       "    0.4026\n",
       "   -3.1716\n",
       "    1.7649\n",
       "   -1.6973\n",
       "   -2.5130\n",
       "   -4.7838\n",
       "    5.5533\n",
       "    0.8169\n",
       "    4.6767\n",
       "   -2.5551\n",
       "   -0.4917\n",
       "   -4.3208\n",
       "    5.1551\n",
       "    2.4426\n",
       "   -4.7315\n",
       "   -2.3107\n",
       "    0.8542\n",
       "    3.1571\n",
       "   -4.3151\n",
       "    4.9621\n",
       "    0.5109\n",
       "    4.9267\n",
       "   -5.4415\n",
       "    1.7678\n",
       "   -5.4462\n",
       "    4.8797\n",
       "    6.1643\n",
       "   -4.5418\n",
       "   -0.4295\n",
       "    6.0222\n",
       "   -5.7429\n",
       "   -0.8246\n",
       "   -4.3188\n",
       "   -0.1266\n",
       "    3.1875\n",
       "   -0.8530\n",
       "    5.6100\n",
       "   -5.3229\n",
       "    4.7129\n",
       "    0.8359\n",
       "    1.6794\n",
       "    2.8484\n",
       "    3.8606\n",
       "    1.6366\n",
       "   -3.1102\n",
       "   -2.8411\n",
       "    1.4702\n",
       "   -2.1311\n",
       "    0.2324\n",
       "   -1.3788\n",
       "   -2.5529\n",
       "    0.5152\n",
       "   -5.3363\n",
       "    4.6477\n",
       "    1.5466\n",
       "    2.9560\n",
       "    4.6126\n",
       "    3.7958\n",
       "   -3.7199\n",
       "   -1.1620\n",
       "    5.0868\n",
       "   -4.2905\n",
       "   -4.2390\n",
       "    2.1505\n",
       "    3.6348\n",
       "    0.6403\n",
       "    5.7604\n",
       "    4.3867\n",
       "    2.8567\n",
       "    5.7743\n",
       "   -2.9110\n",
       "   -6.2464\n",
       "    2.5649\n",
       "   -3.9361\n",
       "   -1.2979\n",
       "    5.3558\n",
       "   -4.9774\n",
       "    5.9072\n",
       "   -5.1418\n",
       "   -3.3690\n",
       "    5.6380\n",
       "    0.2919\n",
       "    1.6529\n",
       "    3.7079\n",
       "    3.2382\n",
       "   -2.8953\n",
       "    1.0092\n",
       "    5.9860\n",
       "    5.3635\n",
       "    0.3518\n",
       "   -2.7255\n",
       "    4.4563\n",
       "    1.9650\n",
       "   -0.2682\n",
       "   -5.6369\n",
       "   -2.2178\n",
       "    2.2848\n",
       "   -4.2419\n",
       "   -1.5518\n",
       "    4.9217\n",
       "    3.3720\n",
       "   -0.4467\n",
       "    4.5113\n",
       "    5.9005\n",
       "    1.7493\n",
       "    5.1855\n",
       "    5.0184\n",
       "    1.6990\n",
       "   -3.3466\n",
       "    0.5819\n",
       "    3.5314\n",
       "   -6.2449\n",
       "   -0.5314\n",
       "   -5.3091\n",
       "    0.1599\n",
       "   -1.9399\n",
       "    1.8260\n",
       "    4.7267\n",
       "   -5.5759\n",
       "   -0.6079\n",
       "   -6.2458\n",
       "   -0.8096\n",
       "   -3.4532\n",
       "    1.5417\n",
       "   -1.4186\n",
       "   -3.8692\n",
       "    0.4105\n",
       "    1.6400\n",
       "    1.0661\n",
       "    3.8713\n",
       "    0.6174\n",
       "    2.0169\n",
       "   -5.3707\n",
       "   -2.7160\n",
       "   -1.4485\n",
       "    3.1698\n",
       "   -3.4491\n",
       "   -2.4430\n",
       "    2.7543\n",
       "   -3.1325\n",
       "    1.8251\n",
       "    4.8754\n",
       "   -4.8138\n",
       "   -0.4604\n",
       "    1.2430\n",
       "   -2.3622\n",
       "    5.1585\n",
       "   -0.1405\n",
       "    5.7804\n",
       "   -3.9565\n",
       "    2.4416\n",
       "    1.4836\n",
       "    0.1208\n",
       "    0.0167\n",
       "   -1.2858\n",
       "   -3.5174\n",
       "    3.1011\n",
       "    2.7927\n",
       "    1.5941\n",
       "   -0.9952\n",
       "    2.5064\n",
       "   -3.9399\n",
       "   -0.4044\n",
       "    4.8861\n",
       "    3.7449\n",
       "    2.5242\n",
       "   -1.6594\n",
       "    2.4974\n",
       "    3.4678\n",
       "   -1.4620\n",
       "   -0.7986\n",
       "   -3.5317\n",
       "    2.1957\n",
       "   -1.2814\n",
       "    2.8411\n",
       "   -5.1743\n",
       "    3.4587\n",
       "    5.5181\n",
       "   -5.2686\n",
       "   -4.5064\n",
       "    3.6049\n",
       "  [torch.cuda.FloatTensor of size 256 (GPU 0)]),\n",
       " ('Why.weight', Parameter containing:\n",
       "  -1.2675e-02  5.7857e-02  5.2303e-02  ...  -1.0920e-02  5.0158e-02  1.5736e-02\n",
       "   4.8012e-02 -4.0253e-02 -9.9160e-03  ...   4.4542e-02  2.8004e-02 -2.8075e-02\n",
       "  -1.4924e-02  6.9068e-03 -1.6862e-03  ...  -7.5659e-03 -4.0433e-02  4.9062e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "   4.5082e-02 -1.9007e-02  5.0388e-03  ...  -1.9481e-02  2.2709e-03  3.9386e-02\n",
       "   9.8607e-03  5.7271e-02  4.0237e-02  ...   5.4514e-02  4.7471e-02  4.3172e-02\n",
       "  -2.4995e-02  5.9404e-02  2.5310e-02  ...   2.8012e-02  5.5535e-02  2.0010e-03\n",
       "  [torch.cuda.FloatTensor of size 256x256 (GPU 0)]),\n",
       " ('Why.bias', Parameter containing:\n",
       "  1.00000e-02 *\n",
       "   -6.1633\n",
       "    1.7530\n",
       "    5.3777\n",
       "   -3.1199\n",
       "    0.6544\n",
       "    1.3799\n",
       "    4.3553\n",
       "   -1.2346\n",
       "   -0.2012\n",
       "   -3.4429\n",
       "   -3.8252\n",
       "    4.8364\n",
       "   -6.0431\n",
       "    6.0376\n",
       "   -2.2595\n",
       "   -4.0160\n",
       "    4.0784\n",
       "   -3.9662\n",
       "    2.6548\n",
       "   -0.3644\n",
       "   -0.2741\n",
       "   -3.5591\n",
       "   -0.3657\n",
       "    1.1969\n",
       "    2.8665\n",
       "    6.0759\n",
       "   -2.7668\n",
       "   -4.8633\n",
       "    5.5089\n",
       "   -6.1768\n",
       "    3.2499\n",
       "    1.7551\n",
       "    0.4436\n",
       "    2.1751\n",
       "    3.1791\n",
       "    2.6239\n",
       "   -4.3916\n",
       "    2.8101\n",
       "    6.0604\n",
       "   -3.3647\n",
       "    3.8572\n",
       "   -5.8327\n",
       "    5.4816\n",
       "   -4.7282\n",
       "   -4.6660\n",
       "    5.5937\n",
       "    2.7503\n",
       "   -5.6603\n",
       "   -1.7775\n",
       "    4.9895\n",
       "   -1.1668\n",
       "   -1.6673\n",
       "   -0.1364\n",
       "   -5.2331\n",
       "    2.7974\n",
       "    1.3309\n",
       "   -3.8228\n",
       "    5.8700\n",
       "   -2.3036\n",
       "    0.5967\n",
       "    4.6150\n",
       "    6.0686\n",
       "   -2.0671\n",
       "    5.4624\n",
       "    0.9785\n",
       "    2.0218\n",
       "   -4.0077\n",
       "    0.5427\n",
       "    4.9453\n",
       "    2.2563\n",
       "   -6.1968\n",
       "    2.8013\n",
       "    4.9404\n",
       "    4.8077\n",
       "   -3.0114\n",
       "   -3.7693\n",
       "    1.6942\n",
       "   -1.1427\n",
       "    1.5930\n",
       "   -5.6733\n",
       "    2.8332\n",
       "    3.6000\n",
       "   -5.2802\n",
       "   -2.8193\n",
       "   -4.9187\n",
       "   -0.8477\n",
       "   -5.1888\n",
       "   -3.8855\n",
       "    3.3164\n",
       "    2.9740\n",
       "    4.6190\n",
       "    1.1656\n",
       "    1.9084\n",
       "    5.2487\n",
       "    4.4699\n",
       "   -3.7701\n",
       "   -2.4910\n",
       "   -1.5558\n",
       "    1.2830\n",
       "    1.6374\n",
       "   -0.8322\n",
       "    4.0916\n",
       "    1.7658\n",
       "   -2.8377\n",
       "   -5.8175\n",
       "    1.6390\n",
       "    0.3661\n",
       "   -1.3641\n",
       "    0.4638\n",
       "   -4.5132\n",
       "   -1.0171\n",
       "   -3.6434\n",
       "   -2.6431\n",
       "   -4.8112\n",
       "    4.8760\n",
       "    3.4239\n",
       "   -5.3327\n",
       "   -4.9238\n",
       "    2.8033\n",
       "    0.4104\n",
       "   -2.4927\n",
       "    0.2697\n",
       "   -1.2546\n",
       "   -5.3939\n",
       "   -4.7629\n",
       "    5.9130\n",
       "   -0.3849\n",
       "   -5.8079\n",
       "   -4.4242\n",
       "   -2.5962\n",
       "   -5.8845\n",
       "    4.3618\n",
       "    1.2822\n",
       "   -3.8939\n",
       "    1.7127\n",
       "   -3.8560\n",
       "   -4.7581\n",
       "    2.7190\n",
       "   -1.4281\n",
       "    2.5784\n",
       "    2.3595\n",
       "    1.5103\n",
       "   -0.7984\n",
       "    3.6815\n",
       "   -0.4538\n",
       "   -0.4948\n",
       "   -5.5034\n",
       "    0.3617\n",
       "   -3.9851\n",
       "    5.8732\n",
       "    5.7236\n",
       "    6.0085\n",
       "    3.2819\n",
       "    0.9432\n",
       "   -4.9238\n",
       "    2.7494\n",
       "   -1.1431\n",
       "   -5.0821\n",
       "   -4.8742\n",
       "   -3.1786\n",
       "    6.2173\n",
       "    0.7672\n",
       "    5.6879\n",
       "    3.5369\n",
       "    5.2175\n",
       "    3.7263\n",
       "   -0.2593\n",
       "    1.1502\n",
       "   -6.1392\n",
       "    2.8023\n",
       "   -3.9903\n",
       "    4.7621\n",
       "    3.7240\n",
       "   -3.7372\n",
       "   -2.2293\n",
       "   -3.0950\n",
       "   -1.2447\n",
       "   -5.0036\n",
       "   -0.9954\n",
       "    2.2212\n",
       "   -4.7211\n",
       "    4.5331\n",
       "    0.5217\n",
       "    2.9542\n",
       "    0.6296\n",
       "    0.5436\n",
       "   -5.5751\n",
       "   -4.2538\n",
       "    2.6289\n",
       "    2.4159\n",
       "   -2.8298\n",
       "   -3.5825\n",
       "    1.8196\n",
       "    4.3523\n",
       "    1.6624\n",
       "    4.0481\n",
       "   -0.4682\n",
       "    2.2253\n",
       "    5.3028\n",
       "   -3.6480\n",
       "   -1.9583\n",
       "    3.3255\n",
       "    2.6554\n",
       "   -4.9869\n",
       "   -1.3995\n",
       "   -0.6640\n",
       "   -6.1987\n",
       "   -5.1342\n",
       "    0.3344\n",
       "   -4.0812\n",
       "    3.9246\n",
       "    2.3585\n",
       "   -1.4201\n",
       "    3.4881\n",
       "   -3.6920\n",
       "    0.5147\n",
       "    2.8887\n",
       "    2.6977\n",
       "    4.7542\n",
       "    4.8795\n",
       "    1.9043\n",
       "    5.5540\n",
       "   -5.9847\n",
       "    5.7151\n",
       "   -0.6939\n",
       "    1.5858\n",
       "   -1.3431\n",
       "   -2.1167\n",
       "   -2.7755\n",
       "   -0.9208\n",
       "    3.6621\n",
       "   -2.9204\n",
       "   -1.3268\n",
       "    2.4396\n",
       "    2.2980\n",
       "    0.6416\n",
       "    4.7179\n",
       "    1.6596\n",
       "   -2.7686\n",
       "    1.5412\n",
       "   -3.1874\n",
       "    5.0519\n",
       "    4.4615\n",
       "   -2.5163\n",
       "   -4.7105\n",
       "   -2.3457\n",
       "    4.0574\n",
       "   -3.9737\n",
       "   -2.3051\n",
       "   -4.7650\n",
       "   -1.3531\n",
       "   -1.2144\n",
       "    1.2894\n",
       "    0.3358\n",
       "   -4.3826\n",
       "   -1.9111\n",
       "  [torch.cuda.FloatTensor of size 256 (GPU 0)]),\n",
       " ('params.0', Parameter containing:\n",
       "  -1.3683 -0.2019  0.2338  0.1287  1.2942 -1.0528  2.2460  0.8851  0.0440  0.4019\n",
       "  -0.3567  1.1364 -0.7880 -0.0273  1.7241  0.5659  1.3446  0.0200 -2.1855 -0.4687\n",
       "   0.0877 -0.0591  0.6670  0.8993  0.0420  1.1561  0.2663  2.6653 -2.6261  0.6782\n",
       "  -1.1388 -0.3196 -0.9254  1.2569  0.2223 -2.0466  0.1015 -1.0162  1.1825 -0.0898\n",
       "  -0.6917 -1.5481  1.4297  0.0764  0.7466  0.1278  0.0962 -0.1685 -1.4564 -0.2203\n",
       "   0.3167  0.1023  0.3089 -1.3633  0.2665  0.4386  0.2293  0.7121  0.7699  1.5792\n",
       "  -2.0811  0.2404 -1.6634  2.1994 -1.9363 -0.2907 -0.9493  1.0852 -0.9470  0.9662\n",
       "  -0.1256  0.9116 -0.1697  0.8081 -0.7293  0.7084  0.0640 -0.2771  0.5188 -1.0021\n",
       "  -0.9839  0.4190  0.4565 -0.8893  0.7665  0.2114  0.4451  2.0868  1.6829 -0.1796\n",
       "   0.3481 -0.8234 -0.0733  0.1092  0.9836 -2.5036  1.5336 -0.9504  0.0301 -1.7637\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.1', Parameter containing:\n",
       "   0.6218  0.1685  0.7042  1.9691 -0.8530 -1.7453 -1.2028  0.7232  0.0443 -0.2332\n",
       "  -0.9146 -0.0748 -0.8721 -0.0611 -0.1999 -0.3337  0.3162  1.5036 -1.2783  0.1070\n",
       "   0.6008 -2.3714  0.3535 -0.6345  0.6216  1.0937 -1.0701  0.3979  1.1206 -0.4600\n",
       "  -0.0638 -0.9490 -0.2837  0.3207  1.4223 -1.8236  0.2460  0.1580 -1.6066  0.0786\n",
       "   1.2908 -0.2222 -0.7487 -0.6372  1.3183 -1.3797 -0.5136  1.1375  0.8964  1.7610\n",
       "   0.4163 -0.6231  0.1994 -2.0006  0.6380  0.1052 -0.9702 -0.9900  0.1900  0.0601\n",
       "   1.3881 -0.4176  1.6815  0.3626  0.9825  1.0426 -0.5792  1.7192  1.3912 -0.2653\n",
       "  -1.0127 -0.2387 -0.6483  1.4419  0.1899  0.7278 -0.0929  1.0928 -0.2077 -0.0100\n",
       "   0.9787 -0.4994  2.5699 -0.3158  0.2517 -0.7485 -0.0935 -0.4469 -1.0149 -1.1627\n",
       "  -1.0976  0.8483 -0.2540  0.0367 -0.7549 -1.0270 -0.6710  0.1671 -0.2167 -0.6145\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.2', Parameter containing:\n",
       "  -0.1560 -1.0691  1.7651 -0.6822 -1.3864  0.7297  1.0157  0.3528  0.0235 -0.1222\n",
       "  -0.7554  0.1295 -1.2936  0.2935  0.7989  0.7741  0.4890  0.5729 -1.2190  1.0014\n",
       "   1.0025  0.2943  0.7926  0.3902  0.6689 -0.6836  0.3411 -0.6867  0.3719 -0.6465\n",
       "  -1.2451 -0.2581 -0.0282  1.5936 -0.1693  0.4435 -0.2861  0.9201  1.0940  0.2864\n",
       "   0.6989 -0.2435  0.3165 -0.2498  0.2791  0.0355 -0.2999 -0.2720  0.8566  0.2471\n",
       "   0.8487  0.5520  0.5898 -0.6515  1.2638  1.4076 -0.0708  0.0516 -1.0538  1.9459\n",
       "  -0.9764 -0.4422  0.7057  1.1497  0.0539 -1.1998  1.4631  1.3232  0.6107  0.8100\n",
       "  -1.1784 -0.5323 -0.4252 -0.5182  0.5520 -1.7788  0.3393  0.1965  0.0937 -0.3870\n",
       "   0.4333 -1.0164  0.2447  0.0715 -0.0757 -0.4178 -0.6700 -1.0792 -0.3020 -2.2175\n",
       "  -0.1417 -2.0669  0.2411 -1.2633  3.1790  0.5952  1.8030  2.0524  0.8227  0.7686\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.3', Parameter containing:\n",
       "   2.1256  0.3294 -0.1517  0.4605 -0.6481 -1.0480 -0.5215 -0.8769  0.2563  0.7200\n",
       "   0.3662 -0.1609  0.0767 -0.2660  0.3099 -0.6185 -1.5153  0.7805  0.0028 -2.5706\n",
       "  -1.1288  1.1639 -0.4820  2.2511  0.9538  0.1537 -0.0479  0.2514 -0.4439  0.1297\n",
       "  -1.2188  0.3030 -0.0117  0.4769  0.6119  1.0262  0.1297  0.7491  1.0394 -0.3555\n",
       "  -0.0692  1.3910 -1.2350 -0.0456  0.2956 -0.8385  0.4854  0.1819 -0.4345 -0.0081\n",
       "  -1.6636 -0.6406 -1.0585 -0.9587 -0.4360 -1.0683  0.4301 -0.6254 -1.2734  0.0487\n",
       "  -0.0307  2.0632 -0.9392 -0.8982 -0.1079 -0.0966  1.2447 -0.1358  0.8820  0.6465\n",
       "  -0.4708 -0.1914 -1.0207 -1.1796  0.2310  1.5208  0.1381  0.6669 -0.6680 -0.0156\n",
       "  -0.2060  0.8078 -1.0172  0.1102  2.7438 -0.2723 -0.1458 -0.6392  0.6471  0.6205\n",
       "   0.2225 -0.3397 -0.5800  1.4635  0.2303  0.6947 -0.9592 -0.7746  0.5512 -1.3186\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.4', Parameter containing:\n",
       "   0.4025 -0.2058  0.7577 -0.1212  0.3833 -0.4657 -0.1718  0.2760 -0.4558 -0.3972\n",
       "   0.0759 -0.3679  0.0984 -0.4315 -0.8783  2.0013 -0.6401  0.4562 -0.3595  1.3802\n",
       "   0.0705  0.3086  1.4703 -0.5752 -0.9408  0.9759  0.7559 -0.7325  1.2748  1.7387\n",
       "  -1.0314  0.2202  1.2258  1.3738  1.1935  0.6043  0.7899 -0.2524 -0.5568  0.6920\n",
       "   1.4735  0.0048  0.9791  0.3516 -1.8978  0.6791  1.7922 -0.0414 -0.5806  0.7249\n",
       "  -1.2282 -0.3697  0.0474 -1.4321  2.2918  0.8290  0.5152  0.4209 -0.5463  0.7222\n",
       "  -1.2370  1.6991 -0.5379  0.0542 -0.2058  0.0931  1.0936  1.3141 -0.2527 -0.8578\n",
       "  -1.5298  0.2113 -0.8754  0.0736  0.2279  0.0945 -0.8631 -0.3939 -2.0666  0.1879\n",
       "   1.0152  1.0494 -0.5518  0.1958 -0.5952 -0.3042  1.1806  1.6678  0.7270 -2.1461\n",
       "   0.2366 -1.1631 -0.3130  0.8893  0.7729 -1.2823  1.2549  0.0862 -2.3942 -0.3422\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.5', Parameter containing:\n",
       "   0.9232  0.7186  0.7188 -2.9856  0.3595  0.5475  0.3502 -0.3302  0.9577 -2.5342\n",
       "  -0.6814  0.7107 -1.9852  0.8701  0.0500  0.3635 -0.2212  0.3465  0.5416  0.2991\n",
       "  -1.2314 -1.8147  1.1938  0.3979 -0.0978  0.1788  0.0487  0.8100 -0.6259 -0.2994\n",
       "  -0.8298 -0.8751  1.8959  0.2291  0.0462  0.2043  1.1119 -1.5711 -0.4210 -1.0089\n",
       "   0.0707  1.8929  1.1736  0.4970 -1.0843 -1.8120 -1.2827 -2.3753 -2.2915 -0.6956\n",
       "  -1.6139 -0.5911 -0.8732  0.9754  0.4926  0.2804  0.4007 -0.7108  0.0456 -1.8877\n",
       "   0.2174  1.2487  0.2245  0.3845  0.1058 -0.2889  0.5130  0.3756 -1.2143  0.7727\n",
       "   0.2746  0.8765 -0.3134 -0.9925 -1.3396 -1.5035 -0.1068  1.1254  0.5792  0.1770\n",
       "  -0.3660  0.3348 -0.2285  0.2716  0.3163  0.6526  0.5576  0.2664 -0.0034  0.1412\n",
       "   0.5238 -0.6495  0.8801 -1.2532 -0.2288  0.0199  0.8419 -1.2372  0.4708  0.9947\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.6', Parameter containing:\n",
       "  -0.0371 -0.8493  0.8220 -1.0718  0.2889  1.8367 -1.3151  1.1290 -1.7796  0.4946\n",
       "   0.3610 -2.2706  0.2241 -0.8864  2.3777  0.5576 -1.1470 -0.8021  0.7748  0.5470\n",
       "   0.2074 -0.7094 -0.9764  1.0303  0.1497 -0.8173 -0.0245  1.6158 -0.3177  1.1927\n",
       "   0.2353  0.3080  0.0974  0.7423  0.5111  1.6431 -0.5312 -0.6781  0.5368 -1.7174\n",
       "  -0.7623 -1.0002  0.1222 -0.4843  0.3580  0.2967  0.6965  0.0685  0.1320 -1.3084\n",
       "  -0.2666 -1.3120  0.3715 -1.5119 -0.5692  0.7363 -0.9956 -1.8069 -2.0863  0.0680\n",
       "  -0.0422 -0.5078 -0.8151 -0.1826  0.4292  0.6288 -1.4025 -1.2210  0.2893  1.1132\n",
       "  -1.2282  0.2793  1.6117  1.2570  0.8060 -0.9719 -0.1033  0.0649 -0.1350 -1.0500\n",
       "  -0.5335 -0.7846  0.4926  0.2120 -0.3667 -0.1124  0.1090  0.5792 -0.4962  1.0144\n",
       "  -0.3821 -0.4374  0.5291  0.7004  0.3953  0.2419 -0.9301  1.3465  0.6123  0.4508\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.7', Parameter containing:\n",
       "  -0.2693 -0.8229  0.8747  0.4674 -1.4842 -0.5123  0.0872 -2.4894  0.3416  2.5585\n",
       "  -2.6189 -1.4311  0.3367 -1.0107  1.5849  2.1972  0.1017 -0.2664 -2.2299 -1.2143\n",
       "   0.9252  0.0231  1.6141 -1.0882 -0.4209  0.4299 -2.0482 -0.4692 -0.7480 -1.3033\n",
       "  -2.4314 -0.9330 -0.6698  0.3747  0.8747 -1.0020 -0.6500  0.2850 -1.3819  0.7223\n",
       "  -0.0267 -1.3558 -0.0177  0.8848 -0.4916  0.8890 -1.2708 -1.3117 -0.0340 -0.0085\n",
       "  -0.5511 -0.0016 -1.1949  0.1133 -0.7035 -0.7567 -0.0600 -1.0068  0.4848 -0.5081\n",
       "   1.0064 -0.4137  0.8746  0.5050  0.6615 -1.0381  0.6314  0.8454 -1.4046 -0.2312\n",
       "   0.4166  2.1134 -0.3016  1.3432  1.1425 -0.1161 -1.0807 -0.5020  0.0886 -0.4103\n",
       "   0.3634 -0.1713  0.3473  1.1302  0.5611  2.0760  1.1052 -0.5103  0.6542  0.3602\n",
       "  -1.3388  0.6398 -0.9256 -0.0753 -0.6768  1.5881  1.2266 -0.6507  0.8292 -0.4939\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.8', Parameter containing:\n",
       "  -1.4356  0.3106  0.8809 -1.1357 -0.1326 -0.1404 -0.8970 -0.5311 -1.3690  0.2434\n",
       "   1.3465  0.0585  0.2608  0.9672 -0.6483  1.7249  0.0326  0.7273  0.5733 -1.4348\n",
       "   2.3625  1.0497  1.7906 -0.1123 -0.3747 -1.6220  0.3212  0.2452 -0.4247  0.5090\n",
       "  -1.5796 -0.1109  0.7842  1.8463  1.2498  0.1617  0.3621 -0.5281  1.0593 -1.2950\n",
       "   1.1456 -0.9563  1.5568 -0.7783  0.8203  0.4631  0.4238  1.0307 -3.1768  0.1823\n",
       "   0.2485  0.0828 -0.8308 -0.8077 -1.1678 -0.9725 -0.3872  0.3023 -0.8744 -0.2339\n",
       "   0.5632  0.9105  1.0696  2.4606  0.0872 -2.6615 -0.2868  0.9378 -0.3880 -0.2211\n",
       "   1.6885 -0.1065 -0.3068 -3.1447  0.8754 -0.2720  0.2933  1.3313 -0.2773 -0.8195\n",
       "  -0.9348  0.6737 -0.3194 -0.8951  0.5457 -0.5000 -1.8610  0.0058  0.0610 -0.0678\n",
       "   0.0758  0.1460 -0.1587 -2.2966 -2.0590 -0.3759 -0.1688 -0.3118  0.8941  0.3774\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)]),\n",
       " ('params.9', Parameter containing:\n",
       "  -0.2272  0.4617  0.9254  0.3865 -1.5954 -1.1755  0.1558 -1.9939 -1.3317  0.7510\n",
       "   0.6657 -2.5702 -1.2479 -0.3625  1.6401 -1.3088  0.3240  0.0318  1.9674  0.0380\n",
       "   0.2598 -0.6867  0.6195 -0.4920  0.4600  0.2306  1.3583 -1.3694  1.0339  0.1965\n",
       "  -1.5955 -0.3014  2.8831 -0.2277 -1.1638  1.1953 -0.3359 -0.9616  0.1497  0.4547\n",
       "   0.8096  0.0219 -0.9409 -0.8287 -0.7351  0.7003 -1.3293  0.0815 -0.7661  1.2113\n",
       "   0.5611 -0.6519  2.3448 -0.5199  0.6873 -1.1116  0.7422 -0.1167  0.6594 -0.0921\n",
       "   0.7914  0.2383 -0.3699  2.7573  1.6971 -1.3238  2.4118  1.7350 -0.2226  0.1996\n",
       "   1.1820 -0.2606  0.2084  0.8072  0.1376 -0.1854  0.4645 -1.6075  0.3766 -0.4208\n",
       "  -0.9431  0.8331 -0.5366 -1.3827  0.6749 -0.0802 -2.1870 -0.6194 -0.6474 -0.3679\n",
       "   1.5365 -1.1076  0.4727 -0.9303 -0.7983 -0.8232 -0.5806  1.6857  0.1580 -0.2004\n",
       "  [torch.cuda.FloatTensor of size 10x10 (GPU 0)])]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder1.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
